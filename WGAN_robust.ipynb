{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom-defined layers\n",
    "\n",
    "class LocationAdd(layers.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LocationAdd, self).__init__()\n",
    "        w_init = tf.keras.initializers.GlorotNormal()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim,)), trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.add(inputs, self.w)\n",
    "    \n",
    "class RegressionAdd(layers.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(RegressionAdd, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        w_init = tf.keras.initializers.GlorotNormal()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim-1,)), trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_X, input_z = tf.split(inputs, [self.input_dim-1, 1], axis=1)\n",
    "        return tf.concat([input_X, tf.add(tf.reshape(tf.linalg.matvec(input_X, self.w),[-1,1]),input_z)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom-defined constraints\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.ops import math_ops\n",
    "class Constraint(object):\n",
    "    def __call__(self, w):\n",
    "        return w\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "    \n",
    "class Max1Norm(Constraint):\n",
    "    \"\"\"1-Norm weight constraint.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_value=2, axis=0):\n",
    "        self.max_value = max_value\n",
    "        self.axis = axis\n",
    "\n",
    "    def __call__(self, w):\n",
    "        norms = math_ops.reduce_sum(math_ops.abs(w), axis=self.axis, keepdims=True)\n",
    "        desired = K.clip(norms, 0, self.max_value)\n",
    "        return w * (desired / (K.epsilon() + norms))\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'max_value': self.max_value, 'axis': self.axis}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WganError(tf.keras.losses.Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        #y_pred = ops.convert_to_tensor(y_pred)\n",
    "        #y_true = math_ops.cast(y_true, y_pred.dtype)\n",
    "        return K.mean(y_pred - y_true, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN:\n",
    "    \n",
    "    ''' A static model, with fixed input size.\n",
    "        Model should not be defined in train(), so it should not depend on dataset dimension.\n",
    "        Model API has an advantage that it can save weights between different calls of train.\n",
    "        Instead of using tf.Session() as before, where only one training can happen, next will refresh,\n",
    "        using Model() API avoids this, it provides a model which saves weights outside tf.Session()!\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, dim_x, target):\n",
    "        self.dim_x = dim_x\n",
    "        self.target = target\n",
    "        self.generator = self.generator_model(dim_x, target)\n",
    "        self.discriminator = self.discriminator_model(dim_x)\n",
    "\n",
    "    \n",
    "    def generator_model(self, dim, target):\n",
    "        if target == \"location\":\n",
    "            inputs = tf.keras.Input(shape=(dim,))\n",
    "            out = LocationAdd(dim)(inputs)\n",
    "            model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "            return model\n",
    "        elif target == \"cov-matrix\":\n",
    "            inputs = tf.keras.Input(shape=(dim,))\n",
    "            out = layers.Dense(units=dim, use_bias=False)(inputs)\n",
    "            model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "            return model\n",
    "        elif target == \"regression\":\n",
    "            inputs = tf.keras.Input(shape=(dim,))\n",
    "            out = RegressionAdd(dim)(inputs)\n",
    "            model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "            return model\n",
    "    \n",
    "    def discriminator_model(self, dim):        \n",
    "        inputs = tf.keras.Input(shape=(dim,))\n",
    "        dense1 = layers.Dense(units=2*dim, activation=tf.keras.activations.sigmoid, \n",
    "                              kernel_constraint=tf.keras.constraints.MaxNorm(max_value=10, axis=0))(inputs)\n",
    "        dense2 = layers.Dense(units=dim, activation=tf.keras.activations.relu, \n",
    "                              kernel_constraint=tf.keras.constraints.MaxNorm(max_value=1, axis=0))(dense1)\n",
    "        out = layers.Dense(units=1, activation=None, \n",
    "                           kernel_constraint=Max1Norm(max_value=1,axis=0))(dense2)\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def discriminator_loss(real_output, fake_output):\n",
    "        #cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        #real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "        #fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        #total_loss = real_loss + fake_loss\n",
    "        total_loss = WganError()(real_output, fake_output)\n",
    "        return total_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def generator_loss(fake_output):\n",
    "        # cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        # loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "        loss = WganError()(fake_output, tf.zeros_like(fake_output))\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def train(self, dataset, epochs, batch_size, step_size):\n",
    "        self.generator_optimizer = tf.keras.optimizers.RMSprop(step_size)\n",
    "        self.discriminator_optimizer = tf.keras.optimizers.RMSprop(step_size)\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            for i in range(dataset.shape[0]//batch_size):\n",
    "                noise = tf.random.normal([batch_size, self.dim_x])\n",
    "                with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "                    generated = self.generator(noise, training=True)\n",
    "                    real_output = self.discriminator(dataset[i*batch_size:(i+1)*batch_size], training=True)\n",
    "                    fake_output = self.discriminator(generated, training=True)\n",
    "\n",
    "                    gen_loss = WGAN.generator_loss(fake_output)\n",
    "                    disc_loss = WGAN.discriminator_loss(real_output, fake_output)\n",
    "\n",
    "                    gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "                    gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "                    self.generator_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "                    self.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "\n",
    "#             print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "#             A = self.generator.trainable_variables[0].numpy()\n",
    "#             if self.target == \"location\" or self.target == \"regression\":\n",
    "#                 print(A)\n",
    "#             elif self.target == \"cov-matrix\":\n",
    "#                 sigma_hat = np.matmul(A, A.T)\n",
    "#                 print(sigma_hat)\n",
    "#                 print(np.linalg.norm(sigma_hat-np.identity(self.dim_x), ord=2))\n",
    "#             print(\"generator loss:\", gen_loss.numpy(), \"discriminator loss: \", disc_loss.numpy())\n",
    "#             print(np.linalg.norm(self.discriminator.trainable_variables[0].numpy(), ord=1, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance matrix estimation\n",
    "\n",
    "https://stackoverflow.com/questions/56201185/how-to-find-a-variable-by-name-in-tensorflow2-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simulate_cov(N, p, model):    \n",
    "#     A = np.random.uniform(size=(p,p))\n",
    "#     cov = A.T @ A\n",
    "#     data = np.random.normal(size=(N, p)) @ A\n",
    "#     z = np.random.binomial(n=1,p=0.1,size=(N,1))\n",
    "#     if model == \"Cauchy\":\n",
    "#         noise = np.random.standard_cauchy(size=(N,p))\n",
    "#     elif model == \"Normal\":\n",
    "#         noise = np.random.normal(5,size=(N,p))\n",
    "#     elif model == \"Gumbel\":\n",
    "#         noise = np.random.gumbel(size=(N,p)) * 5\n",
    "#     data_perturbed = data * (1-z) + noise * z\n",
    "#     data_perturbed = data_perturbed.astype(np.float32)\n",
    "#     # print(\"sample covariance: \\n\", np.cov(data_perturbed.T))\n",
    "#     # print(\"true covariance: \\n\", cov)\n",
    "    \n",
    "#     batch_size = 32 if N == 100 else 64\n",
    "#     epochs = 200\n",
    "#     step_size = 0.01\n",
    "#     wgan = WGAN(dim_x=p, target=\"cov-matrix\")\n",
    "#     wgan.train(data_perturbed, epochs=epochs, batch_size=batch_size, step_size=step_size)\n",
    "    \n",
    "#     sample_2norm_error = np.round(np.linalg.norm(np.cov(data_perturbed.T)-cov, ord=2), 4)\n",
    "#     res_sample_cov[(N,p,model)].append(sample_2norm_error)\n",
    "#     Ahat = wgan.generator.trainable_variables[0].numpy()\n",
    "#     wgan_error = np.round(np.linalg.norm(Ahat.T@Ahat-cov, ord=2), 4)\n",
    "#     res_wgan_cov[(N,p,model)].append(wgan_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ns = [100, 1000, 5000, 10000]\n",
    "# ps = [10, 20, 40]\n",
    "# models = [\"Cauchy\", \"Normal\", \"Gumbel\"]\n",
    "# res_sample_cov = defaultdict(list)\n",
    "# res_wgan_cov = defaultdict(list)\n",
    "# outfile = open(\"cov_est.txt\", 'w')\n",
    "\n",
    "# for N, p, model in itertools.product(Ns, ps, models):\n",
    "#     t = time.time()\n",
    "#     print(\"N={}, p={}, model={} in progress.\".format(N, p, model))\n",
    "    \n",
    "#     for i in range(10):\n",
    "#         simulate_cov(N, p, model)\n",
    "#     outfile.write(\"N={}, p={}, model={} samp: {}\\n\".format(N, p, model, res_sample_cov[(N,p,model)]))\n",
    "#     outfile.write(\"N={}, p={}, model={} wgan: {}\\n\".format(N, p, model, res_wgan_cov[(N,p,model)]))\n",
    "#     outfile.flush()\n",
    "    \n",
    "#     print(\"N={}, p={}, model={} in done.({:.2f} minutes)\".format(N, p, model, (time.time()-t)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_sample_cov = defaultdict(list)\n",
    "# res_wgan_cov = defaultdict(list)\n",
    "# simulate_cov(100, 10, \"Cauchy\")\n",
    "# print(res_sample_cov)\n",
    "# print(res_wgan_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 1000\n",
    "# p = 4\n",
    "\n",
    "# A = np.random.uniform(size=(p,p))\n",
    "# cov = A.T @ A\n",
    "\n",
    "# data = np.random.normal(size=(N,p)) @ A\n",
    "# noise = np.random.standard_cauchy(size=(N,p))\n",
    "# #noise = np.random.gumbel(size=(N,p)) * 5\n",
    "# z = np.random.binomial(n=1,p=0.1,size=(N,1))\n",
    "# data_perturbed = data * (1-z) + noise * z\n",
    "# data_perturbed = data_perturbed.astype(np.float32)\n",
    "# print(\"sample covariance: \\n\", np.cov(data_perturbed.T))\n",
    "# print(\"true covariance: \\n\", cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 100\n",
    "# batch_size = 32\n",
    "# step_size = 0.01\n",
    "\n",
    "# wgan = WGAN(dim_x=p, target=\"cov-matrix\")\n",
    "# wgan.train(data_perturbed, epochs=epochs, batch_size=batch_size, step_size=step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"2-norm loss, samp: \", np.linalg.norm(np.cov(data_perturbed.T)-cov, ord=2))\n",
    "# Ahat = wgan.generator.trainable_variables[0].numpy()\n",
    "# print(\"2-norm loss, wgan: \", np.linalg.norm(Ahat.T@Ahat - cov, ord=2))\n",
    "# print(\"cov hat: \\n\", A.T @ A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_loc(data_perturbed, N, p, model):    \n",
    "    batch_size = 32 if N == 100 else 64\n",
    "    epochs = 200\n",
    "    step_size = 0.01\n",
    "    wgan = WGAN(dim_x=p, target=\"location\")\n",
    "    wgan.train(data_perturbed, epochs=epochs, batch_size=batch_size, step_size=step_size)\n",
    "#     wgan.train(data_perturbed, epochs=epochs//4, batch_size=batch_size, step_size=step_size/2)\n",
    "#     wgan.train(data_perturbed, epochs=epochs//4, batch_size=batch_size, step_size=step_size/10)\n",
    "    \n",
    "    sample_mean_error = np.round(np.linalg.norm(np.mean(data_perturbed, axis=0)-theta)**2, 4)\n",
    "    res_sample_mean[(N,p,model)].append(sample_mean_error)\n",
    "    wgan_error = np.round(np.linalg.norm(wgan.generator.trainable_variables[0].numpy()-theta)**2, 4)\n",
    "    res_wgan_mean[(N,p,model)].append(wgan_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=100, p=10, model=Cauchy in progress.\n",
      "done\n",
      "\n",
      "done\n",
      "\n",
      "done\n",
      "\n",
      "done\n",
      "\n",
      "done\n",
      "\n",
      "done\n",
      "\n",
      "done\n",
      "\n",
      "done\n",
      "\n",
      "done\n",
      "\n",
      "done\n",
      "\n",
      "N=100, p=10, model=Cauchy in done.(5.50 minutes)\n",
      "N=100, p=10, model=Normal in progress.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8408a287acb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0msimulate_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_perturbed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m#     outfile.write(\"N={}, p={}, model={} samp: {}\\n\".format(N, p, model, res_sample_mean[(N,p,model)]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#     outfile.write(\"N={}, p={}, model={} wgan: {}\\n\".format(N, p, model, res_wgan_mean[(N,p,model)]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-75c6a1799964>\u001b[0m in \u001b[0;36msimulate_loc\u001b[0;34m(data_perturbed, N, p, model)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"location\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_perturbed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#     wgan.train(data_perturbed, epochs=epochs//4, batch_size=batch_size, step_size=step_size/2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#     wgan.train(data_perturbed, epochs=epochs//4, batch_size=batch_size, step_size=step_size/10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-af96b4bb524f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, epochs, batch_size, step_size)\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0mdisc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWGAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                     \u001b[0mgradients_of_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                     \u001b[0mgradients_of_discriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MeanGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    270\u001b[0m     factor = _safe_shape_div(\n\u001b[1;32m    271\u001b[0m         math_ops.reduce_prod(input_shape), math_ops.reduce_prod(output_shape))\n\u001b[0;32m--> 272\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruediv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mtruediv\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mdifferent\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m   \"\"\"\n\u001b[0;32m-> 1069\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_truediv_python3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_truediv_python3\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1006\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_div\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf2.1/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mreal_div\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   7280\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   7281\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RealDiv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7282\u001b[0;31m         tld.op_callbacks, x, y)\n\u001b[0m\u001b[1;32m   7283\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7284\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Ns = [100]\n",
    "ps = [10, 20, 40, 80]\n",
    "models = [\"Cauchy\", \"Normal\", \"Gumbel\"]\n",
    "res_sample_mean = defaultdict(list)\n",
    "res_wgan_mean = defaultdict(list)\n",
    "# outfile = open(\"location_est_new.txt\", 'w')\n",
    "\n",
    "for N, p, model in itertools.product(Ns, ps, models):\n",
    "    np.random.seed(1)\n",
    "    theta = np.repeat(np.array([1,2,3,4,5]), p//5)\n",
    "    data = np.random.normal(size=(N, p)) + theta\n",
    "    # print(\"sample mean: \\n\", np.mean(data, axis=0))\n",
    "    z = np.random.binomial(n=1,p=0.1,size=(N,1))\n",
    "    if model == \"Cauchy\":\n",
    "        noise = np.random.standard_cauchy(size=(N,p))\n",
    "    elif model == \"Normal\":\n",
    "        noise = np.random.normal(2,size=(N,p))\n",
    "    elif model == \"Gumbel\":\n",
    "        noise = np.random.gumbel(size=(N,p))\n",
    "    data_perturbed = data * (1-z) + noise * z\n",
    "    data_perturbed = data_perturbed.astype(np.float32)\n",
    "    # print(\"noisy sample mean: \\n\", np.mean(data_perturbed, axis=0))\n",
    "    \n",
    "    t = time.time()\n",
    "    print(\"N={}, p={}, model={} in progress.\".format(N, p, model))\n",
    "    \n",
    "    for i in range(10):\n",
    "        simulate_loc(data_perturbed, N, p, model)\n",
    "#     outfile.write(\"N={}, p={}, model={} samp: {}\\n\".format(N, p, model, res_sample_mean[(N,p,model)]))\n",
    "#     outfile.write(\"N={}, p={}, model={} wgan: {}\\n\".format(N, p, model, res_wgan_mean[(N,p,model)]))\n",
    "#     outfile.flush()\n",
    "    \n",
    "    print(\"N={}, p={}, model={} in done.({:.2f} minutes)\".format(N, p, model, (time.time()-t)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_sample_mean = defaultdict(list)\n",
    "# res_wgan_mean = defaultdict(list)\n",
    "# simulate_loc(100, 80, \"Gumbel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_reg(N, p, model):    \n",
    "    beta = np.repeat(np.array([1,2,3,4,5]), p//5)\n",
    "    data_X = np.random.normal(size=(N,p))\n",
    "    z = np.random.binomial(n=1,p=0.1,size=(N,))\n",
    "    if model == \"Cauchy\":\n",
    "        noise = (1-z)*np.random.normal(scale=1, size=(N,)) + z*np.random.standard_cauchy(size=(N,))\n",
    "    elif model == \"Normal\":\n",
    "        noise = (1-z)*np.random.normal(scale=1, size=(N,)) + z*np.random.normal(scale=10, size=(N,))\n",
    "    elif model == \"Gumbel\":\n",
    "        noise = (1-z)*np.random.normal(scale=1, size=(N,)) + z*np.random.gumbel(size=(N,))*10\n",
    "    data_y = data_X @ beta + noise\n",
    "    data_y = data_y.reshape([-1,1])\n",
    "    data_reg = np.concatenate([data_X, data_y], axis=1)\n",
    "    data_reg = data_reg.astype(np.float32)\n",
    "    betahat = np.linalg.solve(data_X.T@data_X, (data_X.T@data_y)).reshape(-1)\n",
    "    \n",
    "    batch_size = 32 if N == 100 else 64\n",
    "    epochs = 250\n",
    "    step_size = 0.005\n",
    "    wgan = WGAN(dim_x=p+1, target=\"regression\")\n",
    "    wgan.train(data_reg, epochs=epochs, batch_size=batch_size, step_size=step_size)\n",
    "    \n",
    "    ols_error = np.round(np.linalg.norm(betahat - beta), 4)\n",
    "    res_ols_reg[(N,p,model)].append(ols_error)\n",
    "    betahat_wgan = wgan.generator.trainable_variables[0].numpy()\n",
    "    wgan_error = np.round(np.linalg.norm(betahat_wgan - beta), 4)\n",
    "    res_wgan_reg[(N,p,model)].append(wgan_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = [100, 1000, 5000, 10000]\n",
    "ps = [10, 20, 40, 80]\n",
    "models = [\"Cauchy\", \"Normal\", \"Gumbel\"]\n",
    "res_ols_reg = defaultdict(list)\n",
    "res_wgan_reg = defaultdict(list)\n",
    "outfile = open(\"regression_est.txt\", 'w')\n",
    "\n",
    "for N, p, model in itertools.product(Ns, ps, models):\n",
    "    t = time.time()\n",
    "    print(\"N={}, p={}, model={} in progress.\".format(N, p, model))\n",
    "    \n",
    "    for i in range(10):\n",
    "        simulate_reg(N, p, model)\n",
    "    outfile.write(\"N={}, p={}, model={} samp: {}\\n\".format(N, p, model, res_ols_reg[(N,p,model)]))\n",
    "    outfile.write(\"N={}, p={}, model={} wgan: {}\\n\".format(N, p, model, res_wgan_reg[(N,p,model)]))\n",
    "    outfile.flush()\n",
    "    \n",
    "    print(\"N={}, p={}, model={} in done.({:.2f} minutes)\".format(N, p, model, (time.time()-t)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 1000\n",
    "# p = 4\n",
    "# sigma = 1\n",
    "# beta = np.array([1,2,3,4])\n",
    "\n",
    "# A = np.random.uniform(size=(p,p))\n",
    "# data_X = np.random.normal(size=(N,p)) @ A\n",
    "# z = np.random.binomial(n=1,p=0.1,size=(N,))\n",
    "# noise = (1-z)*np.random.normal(scale=sigma, size=(N,)) + z*np.random.standard_cauchy(size=(N,))\n",
    "# # noise = (1-z)*np.random.normal(scale=sigma, size=(N,)) + z*np.random.gumbel(size=(N,))*10\n",
    "# # noise = (1-z)*np.random.normal(scale=sigma, size=(N,)) + z*np.random.normal(scale=10*sigma, size=(N,))\n",
    "# data_y = data_X @ beta + noise\n",
    "# data_y = data_y.reshape([-1,1])\n",
    "# data_reg = np.concatenate([data_X, data_y], axis=1)\n",
    "# data_reg = data_reg.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# betahat = np.linalg.solve(data_X.T@data_X, (data_X.T@data_y)).reshape(-1)\n",
    "# print(\"least square estimate: \\n\", betahat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 200\n",
    "# batch_size = 64\n",
    "# step_size = 0.005\n",
    "\n",
    "# wgan = WGAN(dim_x=p+1, target=\"regression\")\n",
    "# wgan.train(data_reg, epochs=epochs, batch_size=batch_size, step_size=step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"OLS error: \", np.round(np.linalg.norm(betahat - beta), 4))\n",
    "# betahat_wgan = wgan.generator.trainable_variables[0].numpy()\n",
    "# print(\"wgan error: \", np.round(np.linalg.norm(betahat_wgan - beta), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct test version of model with self defined layers\n",
    "# def build_model():\n",
    "#     a = tf.keras.Input(shape=(4,))\n",
    "#     out = LocationAdd(input_dim=4)(a+5)\n",
    "#     model = tf.keras.Model(inputs=a, outputs=out)\n",
    "#     return model\n",
    "# model = build_model()\n",
    "# model2 = build_model()\n",
    "# print(model.trainable_variables)\n",
    "# print(model2.trainable_variables)\n",
    "# model.compile(optimizer='rmsprop', loss=tf.keras.losses.MeanSquaredError())\n",
    "# model.fit(x=data,y=data, batch_size=1, epochs=100)\n",
    "# print(model.trainable_variables)\n",
    "# print(model2.trainable_variables)\n",
    "\n",
    "## tf.keras.layers.add can make variables not trainable, below is not correct\n",
    "# a = tf.keras.Input(shape=(4,))\n",
    "# b = tf.Variable(initial_value=tf.random_normal_initializer()(shape=(4,)), trainable=True)\n",
    "# out = tf.keras.layers.add([a+5,b])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
