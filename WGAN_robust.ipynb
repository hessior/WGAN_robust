{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom-defined layers\n",
    "\n",
    "class LocationAdd(layers.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LocationAdd, self).__init__()\n",
    "        w_init = tf.keras.initializers.GlorotNormal()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim,)), trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.add(inputs, self.w)\n",
    "    \n",
    "class RegressionAdd(layers.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(RegressionAdd, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        w_init = tf.keras.initializers.GlorotNormal()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim-1,)), trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_X, input_z = tf.split(inputs, [self.input_dim-1, 1], axis=1)\n",
    "        return tf.concat([input_X, tf.add(tf.reshape(tf.linalg.matvec(input_X, self.w),[-1,1]),input_z)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom-defined constraints\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.ops import math_ops\n",
    "class Constraint(object):\n",
    "    def __call__(self, w):\n",
    "        return w\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "    \n",
    "class Max1Norm(Constraint):\n",
    "    \"\"\"1-Norm weight constraint.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_value=2, axis=0):\n",
    "        self.max_value = max_value\n",
    "        self.axis = axis\n",
    "\n",
    "    def __call__(self, w):\n",
    "        norms = math_ops.reduce_sum(math_ops.abs(w), axis=self.axis, keepdims=True)\n",
    "        desired = K.clip(norms, 0, self.max_value)\n",
    "        return w * (desired / (K.epsilon() + norms))\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'max_value': self.max_value, 'axis': self.axis}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WganError(tf.keras.losses.Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        #y_pred = ops.convert_to_tensor(y_pred)\n",
    "        #y_true = math_ops.cast(y_true, y_pred.dtype)\n",
    "        return K.mean(y_pred - y_true, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN:\n",
    "    \n",
    "    ''' A static model, with fixed input size.\n",
    "        Model should not be defined in train(), so it should not depend on dataset dimension.\n",
    "        Model API has an advantage that it can save weights between different calls of train.\n",
    "        Instead of using tf.Session() as before, where only one training can happen, next will refresh,\n",
    "        using Model() API avoids this, it provides a model which saves weights outside tf.Session()!\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, dim_x, target):\n",
    "        self.dim_x = dim_x\n",
    "        self.target = target\n",
    "        self.generator = self.generator_model(dim_x, target)\n",
    "        self.discriminator = self.discriminator_model(dim_x)\n",
    "\n",
    "    \n",
    "    def generator_model(self, dim, target):\n",
    "        if target == \"location\":\n",
    "            inputs = tf.keras.Input(shape=(dim,))\n",
    "            out = LocationAdd(dim)(inputs)\n",
    "            model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "            return model\n",
    "        elif target == \"cov-matrix\":\n",
    "            inputs = tf.keras.Input(shape=(dim,))\n",
    "            out = layers.Dense(units=dim, use_bias=False)(inputs)\n",
    "            model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "            return model\n",
    "        elif target == \"regression\":\n",
    "            inputs = tf.keras.Input(shape=(dim,))\n",
    "            out = RegressionAdd(dim)(inputs)\n",
    "            model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "            return model\n",
    "    \n",
    "    def discriminator_model(self, dim):        \n",
    "        inputs = tf.keras.Input(shape=(dim,))\n",
    "        dense1 = layers.Dense(units=2*dim, activation=tf.keras.activations.sigmoid, \n",
    "                              kernel_constraint=tf.keras.constraints.MaxNorm(max_value=1, axis=0))(inputs)\n",
    "        dense2 = layers.Dense(units=dim, activation=tf.keras.activations.relu, \n",
    "                              kernel_constraint=tf.keras.constraints.MaxNorm(max_value=1, axis=0))(dense1)\n",
    "        out = layers.Dense(units=1, activation=None, \n",
    "                           kernel_constraint=Max1Norm(max_value=1,axis=0))(dense2)\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def discriminator_loss(real_output, fake_output):\n",
    "        #cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        #real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "        #fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        #total_loss = real_loss + fake_loss\n",
    "        total_loss = WganError()(real_output, fake_output)\n",
    "        return total_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def generator_loss(fake_output):\n",
    "        # cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        # loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "        loss = WganError()(fake_output, tf.zeros_like(fake_output))\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def train(self, dataset, epochs, batch_size, step_size):\n",
    "        self.generator_optimizer = tf.keras.optimizers.RMSprop(step_size)\n",
    "        self.discriminator_optimizer = tf.keras.optimizers.RMSprop(step_size)\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            for i in range(dataset.shape[0]//batch_size):\n",
    "                noise = tf.random.normal([batch_size, self.dim_x])\n",
    "                with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "                    generated = self.generator(noise, training=True)\n",
    "                    real_output = self.discriminator(dataset[i*batch_size:(i+1)*batch_size], training=True)\n",
    "                    fake_output = self.discriminator(generated, training=True)\n",
    "\n",
    "                    gen_loss = WGAN.generator_loss(fake_output)\n",
    "                    disc_loss = WGAN.discriminator_loss(real_output, fake_output)\n",
    "\n",
    "                    gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "                    gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "                    self.generator_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "                    self.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "\n",
    "#             print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "#             A = self.generator.trainable_variables[0].numpy()\n",
    "#             if self.target == \"location\" or self.target == \"regression\":\n",
    "#                 print(A)\n",
    "#             elif self.target == \"cov-matrix\":\n",
    "#                 sigma_hat = np.matmul(A, A.T)\n",
    "#                 print(sigma_hat)\n",
    "#                 print(np.linalg.norm(sigma_hat-np.identity(self.dim_x), ord=2))\n",
    "#             print(\"generator loss:\", gen_loss.numpy(), \"discriminator loss: \", disc_loss.numpy())\n",
    "#             print(np.linalg.norm(self.discriminator.trainable_variables[0].numpy(), ord=1, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance matrix estimation\n",
    "\n",
    "https://stackoverflow.com/questions/56201185/how-to-find-a-variable-by-name-in-tensorflow2-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_cov(N, p, model):    \n",
    "    A = np.random.uniform(size=(p,p))\n",
    "    cov = A.T @ A\n",
    "    data = np.random.normal(size=(N, p)) @ A\n",
    "    z = np.random.binomial(n=1,p=0.1,size=(N,1))\n",
    "    if model == \"Cauchy\":\n",
    "        noise = np.random.standard_cauchy(size=(N,p))\n",
    "    elif model == \"Normal\":\n",
    "        noise = np.random.normal(5,size=(N,p))\n",
    "    elif model == \"Gumbel\":\n",
    "        noise = np.random.gumbel(size=(N,p)) * 5\n",
    "    data_perturbed = data * (1-z) + noise * z\n",
    "    data_perturbed = data_perturbed.astype(np.float32)\n",
    "    # print(\"sample covariance: \\n\", np.cov(data_perturbed.T))\n",
    "    # print(\"true covariance: \\n\", cov)\n",
    "    \n",
    "    batch_size = 32 if N == 100 else 64\n",
    "    epochs = 200\n",
    "    step_size = 0.01\n",
    "    wgan = WGAN(dim_x=p, target=\"cov-matrix\")\n",
    "    wgan.train(data_perturbed, epochs=epochs, batch_size=batch_size, step_size=step_size)\n",
    "    \n",
    "    sample_2norm_error = np.round(np.linalg.norm(np.cov(data_perturbed.T)-cov, ord=2), 4)\n",
    "    res_sample_cov[(N,p,model)].append(sample_2norm_error)\n",
    "    Ahat = wgan.generator.trainable_variables[0].numpy()\n",
    "    wgan_error = np.round(np.linalg.norm(Ahat.T@Ahat-cov, ord=2), 4)\n",
    "    res_wgan_cov[(N,p,model)].append(wgan_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = [100, 1000, 5000, 10000]\n",
    "ps = [10, 20, 40]\n",
    "models = [\"Cauchy\", \"Normal\", \"Gumbel\"]\n",
    "res_sample_cov = defaultdict(list)\n",
    "res_wgan_cov = defaultdict(list)\n",
    "outfile = open(\"cov_est.txt\", 'w')\n",
    "\n",
    "for N, p, model in itertools.product(Ns, ps, models):\n",
    "    t = time.time()\n",
    "    print(\"N={}, p={}, model={} in progress.\".format(N, p, model))\n",
    "    \n",
    "    for i in range(10):\n",
    "        simulate_cov(N, p, model)\n",
    "    outfile.write(\"N={}, p={}, model={} samp: {}\\n\".format(N, p, model, res_sample_cov[(N,p,model)]))\n",
    "    outfile.write(\"N={}, p={}, model={} wgan: {}\\n\".format(N, p, model, res_wgan_cov[(N,p,model)]))\n",
    "    outfile.flush()\n",
    "    \n",
    "    print(\"N={}, p={}, model={} in done.({:.2f} minutes)\".format(N, p, model, (time.time()-t)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_sample_cov = defaultdict(list)\n",
    "# res_wgan_cov = defaultdict(list)\n",
    "# simulate_cov(100, 10, \"Cauchy\")\n",
    "# print(res_sample_cov)\n",
    "# print(res_wgan_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 1000\n",
    "# p = 4\n",
    "\n",
    "# A = np.random.uniform(size=(p,p))\n",
    "# cov = A.T @ A\n",
    "\n",
    "# data = np.random.normal(size=(N,p)) @ A\n",
    "# noise = np.random.standard_cauchy(size=(N,p))\n",
    "# #noise = np.random.gumbel(size=(N,p)) * 5\n",
    "# z = np.random.binomial(n=1,p=0.1,size=(N,1))\n",
    "# data_perturbed = data * (1-z) + noise * z\n",
    "# data_perturbed = data_perturbed.astype(np.float32)\n",
    "# print(\"sample covariance: \\n\", np.cov(data_perturbed.T))\n",
    "# print(\"true covariance: \\n\", cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 100\n",
    "# batch_size = 32\n",
    "# step_size = 0.01\n",
    "\n",
    "# wgan = WGAN(dim_x=p, target=\"cov-matrix\")\n",
    "# wgan.train(data_perturbed, epochs=epochs, batch_size=batch_size, step_size=step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"2-norm loss, samp: \", np.linalg.norm(np.cov(data_perturbed.T)-cov, ord=2))\n",
    "# Ahat = wgan.generator.trainable_variables[0].numpy()\n",
    "# print(\"2-norm loss, wgan: \", np.linalg.norm(Ahat.T@Ahat - cov, ord=2))\n",
    "# print(\"cov hat: \\n\", A.T @ A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simulate_loc(N, p, model):    \n",
    "#     theta = np.repeat(np.array([1,2,3,4,5]), p//5)\n",
    "#     data = np.random.normal(size=(N, p)) + theta\n",
    "#     # print(\"sample mean: \\n\", np.mean(data, axis=0))\n",
    "#     z = np.random.binomial(n=1,p=0.1,size=(N,1))\n",
    "#     if model == \"Cauchy\":\n",
    "#         noise = np.random.standard_cauchy(size=(N,p))\n",
    "#     elif model == \"Normal\":\n",
    "#         noise = np.random.normal(2,size=(N,p))\n",
    "#     elif model == \"Gumbel\":\n",
    "#         noise = np.random.gumbel(size=(N,p))\n",
    "#     # A = np.random.uniform(size=(p,p))\n",
    "#     # noise = np.random.normal(size=(N,p)) @ A\n",
    "#     data_perturbed = data * (1-z) + noise * z\n",
    "#     data_perturbed = data_perturbed.astype(np.float32)\n",
    "#     # print(\"noisy sample mean: \\n\", np.mean(data_perturbed, axis=0))\n",
    "    \n",
    "#     batch_size = 32 if N == 100 else 64\n",
    "#     epochs = 200\n",
    "#     step_size = 0.01\n",
    "#     wgan = WGAN(dim_x=p, target=\"location\")\n",
    "#     wgan.train(data_perturbed, epochs=epochs, batch_size=batch_size, step_size=step_size)\n",
    "#     wgan.train(data_perturbed, epochs=epochs//4, batch_size=batch_size, step_size=step_size/2)\n",
    "#     wgan.train(data_perturbed, epochs=epochs//4, batch_size=batch_size, step_size=step_size/10)\n",
    "    \n",
    "#     sample_mean_error = np.round(np.linalg.norm(np.mean(data_perturbed, axis=0)-theta)**2, 4)\n",
    "#     res_sample_mean[(N,p,model)].append(sample_mean_error)\n",
    "#     wgan_error = np.round(np.linalg.norm(wgan.generator.trainable_variables[0].numpy()-theta)**2, 4)\n",
    "#     res_wgan_mean[(N,p,model)].append(wgan_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ns = [100, 1000, 5000, 10000]\n",
    "# ps = [10, 20, 40, 80]\n",
    "# models = [\"Cauchy\", \"Normal\", \"Gumbel\"]\n",
    "# res_sample_mean = defaultdict(list)\n",
    "# res_wgan_mean = defaultdict(list)\n",
    "# outfile = open(\"location_est.txt\", 'w')\n",
    "\n",
    "# for N, p, model in itertools.product(Ns, ps, models):\n",
    "#     t = time.time()\n",
    "#     print(\"N={}, p={}, model={} in progress.\".format(N, p, model))\n",
    "    \n",
    "#     for i in range(10):\n",
    "#         simulate_loc(N, p, model)\n",
    "#     outfile.write(\"N={}, p={}, model={} samp: {}\\n\".format(N, p, model, res_sample_mean[(N,p,model)]))\n",
    "#     outfile.write(\"N={}, p={}, model={} wgan: {}\\n\".format(N, p, model, res_wgan_mean[(N,p,model)]))\n",
    "#     outfile.flush()\n",
    "    \n",
    "#     print(\"N={}, p={}, model={} in done.({:.2f} minutes)\".format(N, p, model, (time.time()-t)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_sample_mean = defaultdict(list)\n",
    "# res_wgan_mean = defaultdict(list)\n",
    "# simulate_loc(100, 80, \"Gumbel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 1000\n",
    "# p = 4\n",
    "# sigma = 1\n",
    "# beta = np.array([1,2,3,4], dtype=np.float32)\n",
    "\n",
    "# data_X = np.random.normal(size=(N,p)).astype(np.float32)\n",
    "# data_y = data_X @ beta + np.random.normal(scale=sigma, size=(N,)).astype(np.float32)\n",
    "# data_y = data_y.reshape([-1,1])\n",
    "# data_reg = np.concatenate([data_X, data_y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# betahat = np.linalg.solve(data_X.T@data_X, (data_X.T@data_y))\n",
    "# print(\"least square estimate: \\n\", betahat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 10\n",
    "# batch_size = 32\n",
    "# step_size = 0.01\n",
    "\n",
    "# wgan = WGAN(dim_x=p+1, target=\"regression\")\n",
    "# wgan.train(data_reg, epochs=epochs, batch_size=batch_size, step_size=step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wgan.train(data, epochs=10, batch_size=32, step_size=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct test version of model with self defined layers\n",
    "# def build_model():\n",
    "#     a = tf.keras.Input(shape=(4,))\n",
    "#     out = LocationAdd(input_dim=4)(a+5)\n",
    "#     model = tf.keras.Model(inputs=a, outputs=out)\n",
    "#     return model\n",
    "# model = build_model()\n",
    "# model2 = build_model()\n",
    "# print(model.trainable_variables)\n",
    "# print(model2.trainable_variables)\n",
    "# model.compile(optimizer='rmsprop', loss=tf.keras.losses.MeanSquaredError())\n",
    "# model.fit(x=data,y=data, batch_size=1, epochs=100)\n",
    "# print(model.trainable_variables)\n",
    "# print(model2.trainable_variables)\n",
    "\n",
    "## tf.keras.layers.add can make variables not trainable, below is not correct\n",
    "# a = tf.keras.Input(shape=(4,))\n",
    "# b = tf.Variable(initial_value=tf.random_normal_initializer()(shape=(4,)), trainable=True)\n",
    "# out = tf.keras.layers.add([a+5,b])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
