{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "#import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationAdd(layers.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LocationAdd, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim,), dtype='float32'), trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.add(inputs, self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN:\n",
    "    \n",
    "    ''' A static model, with fixed input size.\n",
    "        Model should not be defined in train(), so it should not depend on dataset dimension.\n",
    "        Model API has an advantage that it can save weights between different calls of train.\n",
    "        Instead of using tf.Session() as before, where only one training can happen, next will refresh,\n",
    "        using Model() API avoids this, it provides a model which saves weights outside tf.Session()!\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, dim_x):\n",
    "        self.dim_x = dim_x\n",
    "        self.generator = self.generator_model(dim_x)\n",
    "        self.discriminator = self.discriminator_model(dim_x)\n",
    "\n",
    "    \n",
    "    def generator_model(self, dim):\n",
    "        inputs = layers.Input(shape=(dim,))\n",
    "        out = LocationAdd(dim)(inputs)\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "        return model\n",
    "    \n",
    "    def discriminator_model(self, dim):\n",
    "        inputs = layers.Input(shape=(dim,))\n",
    "        dense1 = layers.Dense(2*dim, activation=tf.nn.sigmoid)(inputs)\n",
    "        out = layers.Dense(1, activation=tf.nn.sigmoid)(dense1)\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def discriminator_loss(real_output, fake_output):\n",
    "        cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        total_loss = real_loss + fake_loss\n",
    "        return total_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def generator_loss(fake_output):\n",
    "        cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "        \n",
    "       \n",
    "    def train(self, dataset, epochs, batch_size, step_size):\n",
    "        self.generator_optimizer = tf.keras.optimizers.Adam(step_size)\n",
    "        self.discriminator_optimizer = tf.keras.optimizers.Adam(step_size)\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            for i in range(dataset.shape[0]//batch_size):\n",
    "                noise = tf.random.normal([batch_size, self.dim_x])\n",
    "                with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "                    generated = self.generator(noise, training=True)\n",
    "                    real_output = self.discriminator(dataset[i*batch_size:(i+1)*batch_size], training=True)\n",
    "                    fake_output = self.discriminator(generated, training=True)\n",
    "\n",
    "                    gen_loss = WGAN.generator_loss(fake_output)\n",
    "                    disc_loss = WGAN.discriminator_loss(real_output, fake_output)\n",
    "\n",
    "                    gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "                    gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "                    self.generator_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "                    self.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "    \n",
    "            print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "            print(self.generator.trainable_variables[0].numpy())\n",
    "            #print(\"generator loss:\", gen_loss.numpy(), \"discriminator loss: \", disc_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93648027, 1.978882  , 2.84213983, 4.09846861])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.random.normal(size=(100,4)) + np.array([1,2,3,4])\n",
    "np.mean(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan = WGAN(dim_x=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Time for epoch 1 is 3.6977691650390625 sec\n",
      "[ 0.02811069 -0.02587136  0.03036069  0.00514906]\n",
      "Time for epoch 2 is 3.4377169609069824 sec\n",
      "[ 0.02911377 -0.02486555  0.031369    0.00614878]\n",
      "Time for epoch 3 is 3.5907018184661865 sec\n",
      "[ 0.03010978 -0.02385991  0.03237041  0.00716677]\n",
      "Time for epoch 4 is 3.810439109802246 sec\n",
      "[ 0.03111349 -0.02285625  0.03338799  0.00818473]\n",
      "Time for epoch 5 is 3.245352029800415 sec\n",
      "[ 0.03211852 -0.0218489   0.03441513  0.00922087]\n",
      "Time for epoch 6 is 3.166031837463379 sec\n",
      "[ 0.03312031 -0.02083381  0.03545093  0.01025299]\n",
      "Time for epoch 7 is 3.2040462493896484 sec\n",
      "[ 0.03412016 -0.01982057  0.03648442  0.01129337]\n",
      "Time for epoch 8 is 3.3208160400390625 sec\n",
      "[ 0.03513197 -0.01880732  0.03750935  0.01234904]\n",
      "Time for epoch 9 is 3.757997989654541 sec\n",
      "[ 0.03615352 -0.01779163  0.03851769  0.01342299]\n",
      "Time for epoch 10 is 3.619537115097046 sec\n",
      "[ 0.03718481 -0.01676425  0.0395706   0.0145184 ]\n"
     ]
    }
   ],
   "source": [
    "wgan.train(data, 10, 10, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11304637,  0.1813672 ,  0.06781566, -0.09694262], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgan.generator.trainable_variables[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 1 is 3.7638468742370605 sec\n",
      "[0.13746914 0.08385    0.14084247 0.1158528 ]\n",
      "Time for epoch 2 is 3.9754319190979004 sec\n",
      "[0.24260731 0.18970871 0.24846962 0.22440149]\n",
      "Time for epoch 3 is 3.3422088623046875 sec\n",
      "[0.35535112 0.3032939  0.3636259  0.34145534]\n",
      "Time for epoch 4 is 3.193582057952881 sec\n",
      "[0.4696641  0.42172754 0.48498908 0.46409452]\n",
      "Time for epoch 5 is 3.183464765548706 sec\n",
      "[0.58293366 0.544464   0.6132251  0.5924657 ]\n",
      "Time for epoch 6 is 3.151916027069092 sec\n",
      "[0.683527   0.6688246  0.74297917 0.7250354 ]\n",
      "Time for epoch 7 is 4.061507940292358 sec\n",
      "[0.7425081  0.78306454 0.86300606 0.8577488 ]\n",
      "Time for epoch 8 is 3.815953016281128 sec\n",
      "[0.7360032  0.8851273  0.97896034 0.99006027]\n",
      "Time for epoch 9 is 3.9077441692352295 sec\n",
      "[0.6628707 0.9772671 1.0994866 1.128501 ]\n",
      "Time for epoch 10 is 3.224695920944214 sec\n",
      "[0.55274636 1.0543734  1.220586   1.2663777 ]\n"
     ]
    }
   ],
   "source": [
    "wgan.train(data, 10, 10, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 1 is 3.673401117324829 sec\n",
      "[0.55174184 1.0553634  1.2215897  1.2673678 ]\n",
      "Time for epoch 2 is 3.4440970420837402 sec\n",
      "[0.5507361 1.0563568 1.222593  1.2683685]\n",
      "Time for epoch 3 is 3.53598690032959 sec\n",
      "[0.54972816 1.0573869  1.2236145  1.2693708 ]\n",
      "Time for epoch 4 is 3.1795761585235596 sec\n",
      "[0.5487161 1.058404  1.2246329 1.2703743]\n",
      "Time for epoch 5 is 3.1852569580078125 sec\n",
      "[0.5477174 1.0593982 1.2256329 1.2713748]\n",
      "Time for epoch 6 is 3.2666141986846924 sec\n",
      "[0.5467418 1.0603408 1.2266039 1.2723691]\n",
      "Time for epoch 7 is 3.986191749572754 sec\n",
      "[0.5457898 1.0612903 1.2275658 1.2733355]\n",
      "Time for epoch 8 is 4.065059185028076 sec\n",
      "[0.54484683 1.0621921  1.2285038  1.2742845 ]\n",
      "Time for epoch 9 is 4.0624473094940186 sec\n",
      "[0.5439114 1.06309   1.2294383 1.2752315]\n",
      "Time for epoch 10 is 4.251392126083374 sec\n",
      "[0.54292923 1.06405    1.2304255  1.2762266 ]\n"
     ]
    }
   ],
   "source": [
    "wgan.train(data, 10, 10, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 1 is 3.52024507522583 sec\n",
      "[0.4438564 1.1600479 1.3298092 1.37583  ]\n",
      "Time for epoch 2 is 3.163336992263794 sec\n",
      "[0.3555892 1.2473421 1.4249778 1.4718797]\n",
      "Time for epoch 3 is 3.1986138820648193 sec\n",
      "[0.2922245 1.3416042 1.525468  1.5727109]\n",
      "Time for epoch 4 is 3.188176155090332 sec\n",
      "[0.2723909 1.4309354 1.6241009 1.6750035]\n",
      "Time for epoch 5 is 3.1690871715545654 sec\n",
      "[0.3037954 1.5044838 1.7115577 1.7684765]\n",
      "Time for epoch 6 is 3.166724920272827 sec\n",
      "[0.4063912 1.5771102 1.806678  1.8742937]\n",
      "Time for epoch 7 is 3.3385846614837646 sec\n",
      "[0.5362746 1.6321973 1.8983239 1.9768611]\n",
      "Time for epoch 8 is 3.68928599357605 sec\n",
      "[0.6713016 1.6723317 1.9910977 2.076459 ]\n",
      "Time for epoch 9 is 3.490811824798584 sec\n",
      "[0.80498004 1.6982911  2.099583   2.1841578 ]\n",
      "Time for epoch 10 is 3.5169661045074463 sec\n",
      "[0.9095998 1.6906655 2.2086623 2.2905853]\n"
     ]
    }
   ],
   "source": [
    "wgan.train(data, 10, 10, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11961144,  0.19694817,  0.08017154, -0.10297753], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgan.generator.trainable_variables[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.discriminator.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct test version of model with self defined layers\n",
    "def build_model():\n",
    "    a = tf.keras.Input(shape=(4,))\n",
    "    out = LocationAdd(input_dim=4)(a+5)\n",
    "    model = tf.keras.Model(inputs=a, outputs=out)\n",
    "    return model\n",
    "model = build_model()\n",
    "model2 = build_model()\n",
    "print(model.trainable_variables)\n",
    "print(model2.trainable_variables)\n",
    "model.compile(optimizer='rmsprop', loss=tf.keras.losses.MeanSquaredError())\n",
    "model.fit(x=data,y=data, batch_size=1, epochs=100)\n",
    "print(model.trainable_variables)\n",
    "print(model2.trainable_variables)\n",
    "\n",
    "## tf.keras.layers.add can make variables not trainable, below is not correct\n",
    "# a = tf.keras.Input(shape=(4,))\n",
    "# b = tf.Variable(initial_value=tf.random_normal_initializer()(shape=(4,)), trainable=True)\n",
    "# out = tf.keras.layers.add([a+5,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
